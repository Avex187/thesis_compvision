{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5adaad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import os, shutil\n",
    "import tempfile\n",
    "from os import listdir\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.pyplot import imshow\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import mixed_precision, regularizers\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
    "from tensorflow.keras.layers import Input, Add, Dropout, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.initializers import random_uniform, glorot_uniform, constant, identity, he_normal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, CSVLogger\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.applications import InceptionV3, Xception, MobileNetV3Large,EfficientNetB0,EfficientNetV2B0\n",
    "from resnet import resnet18\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix, matthews_corrcoef\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6a7b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.set_cmap('gray')\n",
    "pd.set_option('precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed21e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gpu in tf.config.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0626dc",
   "metadata": {},
   "source": [
    "## Setting awal beberapa hyperparameter, serta penggunaan *mixed precision* dari NVIDIA CUDA/CU-DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f7df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3060 Laptop GPU, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "def scheduler1(epoch, lr):\n",
    "    if epoch < 6:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.05)\n",
    "\n",
    "def scheduler2(epoch, lr):\n",
    "    if epoch < 6:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "def scheduler3(epoch, lr):\n",
    "    if epoch < 6:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.15)\n",
    "    \n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.005, patience = 6,  restore_best_weights=True)\n",
    "lrs1 = LearningRateScheduler(scheduler1)\n",
    "lrs2 = LearningRateScheduler(scheduler2)\n",
    "lrs3 = LearningRateScheduler(scheduler3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1a8f5",
   "metadata": {},
   "source": [
    "## Data Generator Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba895e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datagen(train_path, val_path, test_path, target_size = (256,256), batch_size = 16, efficient = False):\n",
    "    if efficient:\n",
    "        train_datagen = image.ImageDataGenerator(\n",
    "            rescale = 1.,\n",
    "        )\n",
    "    else:\n",
    "        train_datagen = image.ImageDataGenerator(\n",
    "            rescale = 1./255,\n",
    "        )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=target_size,\n",
    "        batch_size= batch_size,\n",
    "        color_mode=\"rgb\",\n",
    "        class_mode='categorical',\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        val_path,\n",
    "        target_size=target_size,\n",
    "        batch_size= batch_size,\n",
    "        color_mode=\"rgb\",\n",
    "        class_mode='categorical',\n",
    "        shuffle = False\n",
    "    )\n",
    "    \n",
    "    test_generator = train_datagen.flow_from_directory(\n",
    "        test_path,\n",
    "        target_size=target_size,\n",
    "        batch_size= batch_size,\n",
    "        color_mode=\"rgb\",\n",
    "        class_mode='categorical',\n",
    "        shuffle = False\n",
    "    )\n",
    "    return train_generator, validation_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b26bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.HeNormal()\n",
    "\n",
    "def add_regularization(model, regularizer = regularizers.l2(0.01)):\n",
    "    if not isinstance(regularizer, regularizers.Regularizer):\n",
    "        print(\"Regularizer must be a subclass of tf.keras.regularizers.Regularizer\")\n",
    "        return model\n",
    "\n",
    "    for layer in model.layers:\n",
    "        for attr in ['kernel_regularizer']:\n",
    "            if hasattr(layer, attr):\n",
    "                setattr(layer, attr, regularizer)\n",
    "\n",
    "    model_json = model.to_json()\n",
    "\n",
    "    tmp_weights_path = os.path.join(tempfile.gettempdir(), 'tmp_weights.h5')\n",
    "    model.save_weights(tmp_weights_path)\n",
    "\n",
    "    model = models.model_from_json(model_json)\n",
    "    \n",
    "    model.load_weights(tmp_weights_path, by_name=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a4fb4",
   "metadata": {},
   "source": [
    "## Utility pembuat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "129ffb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Inception():\n",
    "    base = InceptionV3(\n",
    "        include_top = False,\n",
    "        weights = \"imagenet\",\n",
    "        input_shape = (256, 256, 3),\n",
    "        pooling = \"max\",\n",
    "        classes = 4,\n",
    "        classifier_activation=\"softmax\",\n",
    "    )\n",
    "    out = Dropout(0.4)(base.output)\n",
    "    out = Dense(32, activation='relu', kernel_initializer=initializer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Dropout(0.4)(out)\n",
    "    out = Dense(4, activation='softmax', kernel_initializer=\"glorot_uniform\")(out)\n",
    "\n",
    "    model = Model(inputs = base.input,outputs=out)\n",
    "    add_regularization(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ab221f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xception():  \n",
    "    base = Xception(\n",
    "        include_top = False,\n",
    "        weights = \"imagenet\",\n",
    "        input_shape = (256, 256, 3),\n",
    "        pooling = \"max\"\n",
    "    )\n",
    "    out = Dropout(0.5)(base.output)\n",
    "    out = Dense(32, activation='relu', kernel_initializer=initializer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Dropout(0.5)(out)\n",
    "    out = Dense(4, activation='softmax', kernel_initializer=\"glorot_uniform\")(out)\n",
    "\n",
    "    model = Model(inputs = base.input,outputs=out)\n",
    "    add_regularization(model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b433ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mobileNet():\n",
    "    base = MobileNetV3Large(\n",
    "        input_shape= (256, 256, 3),\n",
    "        alpha=1.0,\n",
    "        include_top = False,\n",
    "        weights= \"imagenet\",\n",
    "        classes = 4,\n",
    "        pooling = \"max\",\n",
    "        dropout_rate = 0.3,\n",
    "    )\n",
    "    out = Dropout(0.5)(base.output)\n",
    "    out = Dense(32, activation='relu', kernel_initializer=initializer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Dropout(0.5)(out)\n",
    "    out = Dense(4, activation='softmax', kernel_initializer=\"glorot_uniform\")(out)\n",
    "\n",
    "    model = Model(inputs = base.input,outputs=out)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f15bfc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_EfficientB0():\n",
    "    base = EfficientNetB0(\n",
    "        include_top = False,\n",
    "        weights = \"imagenet\",\n",
    "        input_shape = (256, 256, 3),\n",
    "        pooling = \"max\",\n",
    "        classes = 4\n",
    "    )\n",
    "    out = Dropout(0.5)(base.output)\n",
    "    out = Dense(64, activation='relu', kernel_initializer=initializer)(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Dropout(0.5)(out)\n",
    "    out = Dense(4, activation='softmax', kernel_initializer=\"glorot_uniform\")(out)\n",
    "\n",
    "    model = Model(inputs = base.input,outputs=out)\n",
    "    add_regularization(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f1125",
   "metadata": {},
   "source": [
    "## Define Path Dataset + Trial Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bceeea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'Categorized Datasets/Dataset/Distribution 1/Model D/train'\n",
    "val_path = 'Categorized Datasets/Dataset/Distribution 1/Model D/val'\n",
    "test_path = 'Categorized Datasets/Dataset/Distribution 1/Model D/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator, validation_generator, test_generator = create_datagen(train_path, val_path, test_path,\n",
    "                                                                       batch_size = batch,efficient = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbb4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = next(train_generator)\n",
    "\n",
    "plt.figure(figsize=(16, 32))\n",
    "for k, (img, lbl) in enumerate(zip(x_batch, y_batch)):\n",
    "    plt.subplot(8, 4, k+1)#4 rows with 8 images.\n",
    "    plt.title(str(lbl))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bc35b",
   "metadata": {},
   "source": [
    "### Optimisasi untuk Model HQ,HPS,RSS - Inc, Xcp, Mbl, Eff - Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "371162d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 16\n",
      "Found 3480 images belonging to 4 classes.\n",
      "Found 101 images belonging to 4 classes.\n",
      "Found 106 images belonging to 4 classes.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 - 55s - loss: 1.2595 - accuracy: 0.6632 - val_loss: 0.9249 - val_accuracy: 0.8911 - 55s/epoch - 251ms/step\n",
      "Epoch 2/8\n",
      "218/218 - 33s - loss: 0.8109 - accuracy: 0.9069 - val_loss: 0.9598 - val_accuracy: 0.8614 - 33s/epoch - 152ms/step\n",
      "Epoch 3/8\n",
      "218/218 - 34s - loss: 0.7202 - accuracy: 0.9589 - val_loss: 0.8160 - val_accuracy: 0.8713 - 34s/epoch - 155ms/step\n",
      "Epoch 4/8\n",
      "218/218 - 33s - loss: 0.6847 - accuracy: 0.9796 - val_loss: 0.8719 - val_accuracy: 0.8812 - 33s/epoch - 151ms/step\n",
      "Epoch 5/8\n",
      "218/218 - 33s - loss: 0.6686 - accuracy: 0.9851 - val_loss: 0.8016 - val_accuracy: 0.8713 - 33s/epoch - 152ms/step\n",
      "Epoch 6/8\n",
      "218/218 - 33s - loss: 0.6595 - accuracy: 0.9908 - val_loss: 0.8103 - val_accuracy: 0.9109 - 33s/epoch - 152ms/step\n",
      "Epoch 7/8\n",
      "218/218 - 33s - loss: 0.6483 - accuracy: 0.9937 - val_loss: 0.8758 - val_accuracy: 0.8812 - 33s/epoch - 153ms/step\n",
      "Epoch 8/8\n",
      "218/218 - 33s - loss: 0.6495 - accuracy: 0.9920 - val_loss: 0.8880 - val_accuracy: 0.8416 - 33s/epoch - 152ms/step\n",
      "Minimum val loss is 0.8016183035714286\n",
      "(0.8016183035714286, 0.001, 16)\n",
      "0.001 16\n",
      "Found 3480 images belonging to 4 classes.\n",
      "Found 101 images belonging to 4 classes.\n",
      "Found 106 images belonging to 4 classes.\n",
      "Epoch 1/8\n",
      "218/218 - 49s - loss: 1.2445 - accuracy: 0.6621 - val_loss: 0.9228 - val_accuracy: 0.8317 - 49s/epoch - 226ms/step\n",
      "Epoch 2/8\n",
      "218/218 - 33s - loss: 0.8309 - accuracy: 0.8954 - val_loss: 0.8380 - val_accuracy: 0.8713 - 33s/epoch - 152ms/step\n",
      "Epoch 3/8\n",
      "218/218 - 33s - loss: 0.7440 - accuracy: 0.9477 - val_loss: 0.9302 - val_accuracy: 0.8911 - 33s/epoch - 152ms/step\n",
      "Epoch 4/8\n",
      "218/218 - 34s - loss: 0.6896 - accuracy: 0.9779 - val_loss: 0.8521 - val_accuracy: 0.8812 - 34s/epoch - 154ms/step\n",
      "Epoch 5/8\n",
      "218/218 - 34s - loss: 0.6707 - accuracy: 0.9845 - val_loss: 0.9077 - val_accuracy: 0.9208 - 34s/epoch - 156ms/step\n",
      "Epoch 6/8\n",
      "218/218 - 34s - loss: 0.6565 - accuracy: 0.9894 - val_loss: 0.8862 - val_accuracy: 0.8911 - 34s/epoch - 154ms/step\n",
      "Epoch 7/8\n",
      "218/218 - 33s - loss: 0.6518 - accuracy: 0.9922 - val_loss: 0.8072 - val_accuracy: 0.8911 - 33s/epoch - 152ms/step\n",
      "Epoch 8/8\n",
      "218/218 - 33s - loss: 0.6493 - accuracy: 0.9914 - val_loss: 0.8594 - val_accuracy: 0.9307 - 33s/epoch - 152ms/step\n",
      "Minimum val loss is 0.8071986607142857\n",
      "0.01 8\n",
      "Found 3480 images belonging to 4 classes.\n",
      "Found 101 images belonging to 4 classes.\n",
      "Found 106 images belonging to 4 classes.\n",
      "Epoch 1/8\n",
      "435/435 - 76s - loss: 1.3900 - accuracy: 0.3457 - val_loss: 1.1550 - val_accuracy: 0.5842 - 76s/epoch - 174ms/step\n",
      "Epoch 2/8\n",
      "435/435 - 59s - loss: 1.2521 - accuracy: 0.4236 - val_loss: 5.2800 - val_accuracy: 0.1188 - 59s/epoch - 137ms/step\n",
      "Epoch 3/8\n",
      "435/435 - 59s - loss: 1.3292 - accuracy: 0.3635 - val_loss: 1.1024 - val_accuracy: 0.6337 - 59s/epoch - 136ms/step\n",
      "Epoch 4/8\n",
      "435/435 - 59s - loss: 1.3329 - accuracy: 0.3466 - val_loss: 1.1414 - val_accuracy: 0.6337 - 59s/epoch - 137ms/step\n",
      "Epoch 5/8\n",
      "435/435 - 60s - loss: 1.2610 - accuracy: 0.4348 - val_loss: 1.1270 - val_accuracy: 0.3366 - 60s/epoch - 137ms/step\n",
      "Epoch 6/8\n",
      "435/435 - 60s - loss: 1.2252 - accuracy: 0.4713 - val_loss: 1.3989 - val_accuracy: 0.2970 - 60s/epoch - 137ms/step\n",
      "Epoch 7/8\n",
      "435/435 - 60s - loss: 1.1794 - accuracy: 0.5417 - val_loss: 1.1974 - val_accuracy: 0.6040 - 60s/epoch - 138ms/step\n",
      "Epoch 8/8\n",
      "435/435 - 60s - loss: 1.1402 - accuracy: 0.5922 - val_loss: 1.1831 - val_accuracy: 0.6535 - 60s/epoch - 138ms/step\n",
      "Minimum val loss is 1.1023888221153846\n",
      "0.001 16\n",
      "Found 3480 images belonging to 4 classes.\n",
      "Found 101 images belonging to 4 classes.\n",
      "Found 106 images belonging to 4 classes.\n",
      "Epoch 1/8\n",
      "218/218 - 50s - loss: 1.2943 - accuracy: 0.6342 - val_loss: 0.9658 - val_accuracy: 0.8416 - 50s/epoch - 229ms/step\n",
      "Epoch 2/8\n",
      "218/218 - 33s - loss: 0.8327 - accuracy: 0.8940 - val_loss: 0.9978 - val_accuracy: 0.8218 - 33s/epoch - 153ms/step\n",
      "Epoch 3/8\n",
      "218/218 - 33s - loss: 0.7284 - accuracy: 0.9606 - val_loss: 0.9059 - val_accuracy: 0.8515 - 33s/epoch - 153ms/step\n",
      "Epoch 4/8\n",
      "218/218 - 33s - loss: 0.6876 - accuracy: 0.9819 - val_loss: 0.8006 - val_accuracy: 0.8614 - 33s/epoch - 153ms/step\n",
      "Epoch 5/8\n",
      "218/218 - 33s - loss: 0.6663 - accuracy: 0.9879 - val_loss: 0.7932 - val_accuracy: 0.8911 - 33s/epoch - 154ms/step\n",
      "Epoch 6/8\n",
      "218/218 - 33s - loss: 0.6593 - accuracy: 0.9897 - val_loss: 0.8039 - val_accuracy: 0.9010 - 33s/epoch - 154ms/step\n",
      "Epoch 7/8\n",
      "218/218 - 33s - loss: 0.6498 - accuracy: 0.9934 - val_loss: 0.8292 - val_accuracy: 0.8812 - 33s/epoch - 153ms/step\n",
      "Epoch 8/8\n",
      "218/218 - 33s - loss: 0.6455 - accuracy: 0.9931 - val_loss: 0.9309 - val_accuracy: 0.8812 - 33s/epoch - 153ms/step\n",
      "Minimum val loss is 0.7931780133928571\n",
      "(0.7931780133928571, 0.001, 16)\n",
      "0.001 16\n",
      "Found 3480 images belonging to 4 classes.\n",
      "Found 101 images belonging to 4 classes.\n",
      "Found 106 images belonging to 4 classes.\n",
      "Epoch 1/8\n",
      "218/218 - 50s - loss: 1.2724 - accuracy: 0.6707 - val_loss: 1.1926 - val_accuracy: 0.8416 - 50s/epoch - 229ms/step\n",
      "Epoch 2/8\n",
      "218/218 - 34s - loss: 0.8442 - accuracy: 0.8830 - val_loss: 0.9173 - val_accuracy: 0.8119 - 34s/epoch - 156ms/step\n",
      "Epoch 3/8\n",
      "218/218 - 34s - loss: 0.7336 - accuracy: 0.9609 - val_loss: 0.7833 - val_accuracy: 0.8812 - 34s/epoch - 156ms/step\n",
      "Epoch 4/8\n",
      "218/218 - 34s - loss: 0.6990 - accuracy: 0.9796 - val_loss: 0.8234 - val_accuracy: 0.8911 - 34s/epoch - 156ms/step\n",
      "Epoch 5/8\n",
      "218/218 - 34s - loss: 0.6753 - accuracy: 0.9865 - val_loss: 0.8202 - val_accuracy: 0.8911 - 34s/epoch - 155ms/step\n",
      "Epoch 6/8\n",
      "218/218 - 34s - loss: 0.6584 - accuracy: 0.9934 - val_loss: 0.8653 - val_accuracy: 0.8713 - 34s/epoch - 156ms/step\n",
      "Epoch 7/8\n",
      "218/218 - 34s - loss: 0.6519 - accuracy: 0.9940 - val_loss: 0.9046 - val_accuracy: 0.8812 - 34s/epoch - 156ms/step\n",
      "Epoch 8/8\n",
      "218/218 - 34s - loss: 0.6537 - accuracy: 0.9897 - val_loss: 0.8784 - val_accuracy: 0.8911 - 34s/epoch - 155ms/step\n",
      "Minimum val loss is 0.7833426339285714\n",
      "(0.7833426339285714, 0.001, 16)\n",
      "0.001 8\n",
      "Found 3480 images belonging to 4 classes.\n",
      "Found 101 images belonging to 4 classes.\n",
      "Found 106 images belonging to 4 classes.\n",
      "Epoch 1/8\n",
      "435/435 - 76s - loss: 1.1909 - accuracy: 0.6644 - val_loss: 0.9339 - val_accuracy: 0.8911 - 76s/epoch - 175ms/step\n",
      "Epoch 2/8\n",
      "435/435 - 61s - loss: 0.8095 - accuracy: 0.8908 - val_loss: 0.8442 - val_accuracy: 0.8812 - 61s/epoch - 139ms/step\n",
      "Epoch 3/8\n",
      "435/435 - 60s - loss: 0.7443 - accuracy: 0.9417 - val_loss: 0.9925 - val_accuracy: 0.7921 - 60s/epoch - 139ms/step\n",
      "Epoch 4/8\n",
      "435/435 - 60s - loss: 0.7130 - accuracy: 0.9652 - val_loss: 0.8141 - val_accuracy: 0.8812 - 60s/epoch - 139ms/step\n",
      "Epoch 5/8\n",
      "435/435 - 60s - loss: 0.7034 - accuracy: 0.9713 - val_loss: 0.7981 - val_accuracy: 0.9010 - 60s/epoch - 139ms/step\n",
      "Epoch 6/8\n",
      "435/435 - 60s - loss: 0.7044 - accuracy: 0.9667 - val_loss: 0.8379 - val_accuracy: 0.8911 - 60s/epoch - 139ms/step\n",
      "Epoch 7/8\n",
      "435/435 - 60s - loss: 0.6931 - accuracy: 0.9741 - val_loss: 0.8102 - val_accuracy: 0.8911 - 60s/epoch - 138ms/step\n",
      "Epoch 8/8\n",
      "435/435 - 60s - loss: 0.6868 - accuracy: 0.9759 - val_loss: 0.8265 - val_accuracy: 0.8911 - 60s/epoch - 139ms/step\n",
      "Minimum val loss is 0.7980769230769231\n",
      "0.001 32\n",
      "Found 3480 images belonging to 4 classes.\n",
      "Found 101 images belonging to 4 classes.\n",
      "Found 106 images belonging to 4 classes.\n",
      "Epoch 1/8\n",
      "109/109 - 41s - loss: 1.3117 - accuracy: 0.6503 - val_loss: 1.8794 - val_accuracy: 0.8119 - 41s/epoch - 380ms/step\n",
      "Epoch 2/8\n",
      "109/109 - 22s - loss: 0.8256 - accuracy: 0.9161 - val_loss: 0.9884 - val_accuracy: 0.8515 - 22s/epoch - 204ms/step\n",
      "Epoch 3/8\n",
      "109/109 - 22s - loss: 0.7310 - accuracy: 0.9649 - val_loss: 0.9430 - val_accuracy: 0.8812 - 22s/epoch - 204ms/step\n",
      "Epoch 4/8\n",
      "109/109 - 22s - loss: 0.6960 - accuracy: 0.9813 - val_loss: 0.9287 - val_accuracy: 0.8812 - 22s/epoch - 204ms/step\n",
      "Epoch 5/8\n",
      "109/109 - 22s - loss: 0.6667 - accuracy: 0.9905 - val_loss: 0.9149 - val_accuracy: 0.8416 - 22s/epoch - 204ms/step\n",
      "Epoch 6/8\n",
      "109/109 - 22s - loss: 0.6551 - accuracy: 0.9945 - val_loss: 1.0079 - val_accuracy: 0.8713 - 22s/epoch - 204ms/step\n",
      "Epoch 7/8\n",
      "109/109 - 22s - loss: 0.6407 - accuracy: 0.9968 - val_loss: 1.0344 - val_accuracy: 0.8911 - 22s/epoch - 204ms/step\n",
      "Epoch 8/8\n",
      "109/109 - 22s - loss: 0.6369 - accuracy: 0.9963 - val_loss: 1.0193 - val_accuracy: 0.8713 - 22s/epoch - 204ms/step\n",
      "Minimum val loss is 0.9149169921875\n",
      "5e-05 8\n",
      "Found 3480 images belonging to 4 classes.\n",
      "Found 101 images belonging to 4 classes.\n",
      "Found 106 images belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "435/435 - 78s - loss: 2.1460 - accuracy: 0.3023 - val_loss: 1.2524 - val_accuracy: 0.4752 - 78s/epoch - 179ms/step\n",
      "Epoch 2/8\n",
      "435/435 - 62s - loss: 1.8766 - accuracy: 0.3753 - val_loss: 1.0369 - val_accuracy: 0.6535 - 62s/epoch - 142ms/step\n",
      "Epoch 3/8\n",
      "435/435 - 62s - loss: 1.6790 - accuracy: 0.4397 - val_loss: 0.9412 - val_accuracy: 0.7426 - 62s/epoch - 142ms/step\n",
      "Epoch 4/8\n",
      "435/435 - 61s - loss: 1.4879 - accuracy: 0.5037 - val_loss: 0.8987 - val_accuracy: 0.8317 - 61s/epoch - 141ms/step\n",
      "Epoch 5/8\n",
      "435/435 - 61s - loss: 1.3494 - accuracy: 0.5805 - val_loss: 0.8825 - val_accuracy: 0.8515 - 61s/epoch - 141ms/step\n",
      "Epoch 6/8\n",
      "435/435 - 62s - loss: 1.2432 - accuracy: 0.6333 - val_loss: 0.8653 - val_accuracy: 0.8515 - 62s/epoch - 141ms/step\n",
      "Epoch 7/8\n",
      "435/435 - 62s - loss: 1.1676 - accuracy: 0.6770 - val_loss: 0.8702 - val_accuracy: 0.8515 - 62s/epoch - 142ms/step\n",
      "Epoch 8/8\n",
      "435/435 - 62s - loss: 1.1160 - accuracy: 0.7190 - val_loss: 0.8617 - val_accuracy: 0.8515 - 62s/epoch - 142ms/step\n",
      "Minimum val loss is 0.8617037259615384\n",
      "0.0001 16\n",
      "Found 3480 images belonging to 4 classes.\n",
      "Found 101 images belonging to 4 classes.\n",
      "Found 106 images belonging to 4 classes.\n",
      "Epoch 1/8\n",
      "218/218 - 50s - loss: 1.9508 - accuracy: 0.3601 - val_loss: 1.1122 - val_accuracy: 0.6733 - 50s/epoch - 229ms/step\n",
      "Epoch 2/8\n",
      "218/218 - 34s - loss: 1.4596 - accuracy: 0.5236 - val_loss: 1.0003 - val_accuracy: 0.7723 - 34s/epoch - 156ms/step\n",
      "Epoch 3/8\n",
      "218/218 - 34s - loss: 1.2038 - accuracy: 0.6497 - val_loss: 0.9470 - val_accuracy: 0.7822 - 34s/epoch - 156ms/step\n",
      "Epoch 4/8\n",
      "218/218 - 34s - loss: 1.0842 - accuracy: 0.7391 - val_loss: 0.9091 - val_accuracy: 0.8515 - 34s/epoch - 156ms/step\n",
      "Epoch 5/8\n",
      "218/218 - 34s - loss: 1.0294 - accuracy: 0.7747 - val_loss: 0.8978 - val_accuracy: 0.8416 - 34s/epoch - 156ms/step\n",
      "Epoch 6/8\n",
      "218/218 - 34s - loss: 0.9297 - accuracy: 0.8454 - val_loss: 0.8857 - val_accuracy: 0.8614 - 34s/epoch - 156ms/step\n",
      "Epoch 7/8\n",
      "218/218 - 34s - loss: 0.9131 - accuracy: 0.8540 - val_loss: 0.8881 - val_accuracy: 0.8614 - 34s/epoch - 155ms/step\n",
      "Epoch 8/8\n",
      "218/218 - 34s - loss: 0.8665 - accuracy: 0.8954 - val_loss: 0.8906 - val_accuracy: 0.8416 - 34s/epoch - 156ms/step\n",
      "Minimum val loss is 0.8857421875\n",
      "5e-05 8\n",
      "Found 3480 images belonging to 4 classes.\n",
      "Found 101 images belonging to 4 classes.\n",
      "Found 106 images belonging to 4 classes.\n",
      "Epoch 1/8\n",
      "435/435 - 77s - loss: 2.1027 - accuracy: 0.3170 - val_loss: 1.0939 - val_accuracy: 0.5842 - 77s/epoch - 178ms/step\n",
      "Epoch 2/8\n",
      "435/435 - 61s - loss: 1.7610 - accuracy: 0.4261 - val_loss: 0.9716 - val_accuracy: 0.7228 - 61s/epoch - 141ms/step\n",
      "Epoch 3/8\n",
      "435/435 - 62s - loss: 1.5013 - accuracy: 0.5310 - val_loss: 0.9386 - val_accuracy: 0.7525 - 62s/epoch - 141ms/step\n",
      "Epoch 4/8\n",
      "435/435 - 61s - loss: 1.3595 - accuracy: 0.5954 - val_loss: 0.9073 - val_accuracy: 0.7921 - 61s/epoch - 141ms/step\n",
      "Epoch 5/8\n",
      "435/435 - 61s - loss: 1.2268 - accuracy: 0.6629 - val_loss: 0.8795 - val_accuracy: 0.8119 - 61s/epoch - 141ms/step\n",
      "Epoch 6/8\n",
      "435/435 - 62s - loss: 1.1730 - accuracy: 0.6908 - val_loss: 0.8646 - val_accuracy: 0.8812 - 62s/epoch - 141ms/step\n",
      "Epoch 7/8\n",
      "435/435 - 61s - loss: 1.1185 - accuracy: 0.7287 - val_loss: 0.8539 - val_accuracy: 0.8812 - 61s/epoch - 141ms/step\n",
      "Epoch 8/8\n",
      "435/435 - 61s - loss: 1.0671 - accuracy: 0.7612 - val_loss: 0.8283 - val_accuracy: 0.8812 - 61s/epoch - 141ms/step\n",
      "Minimum val loss is 0.8282752403846154\n"
     ]
    }
   ],
   "source": [
    "best_hyperparam = (100, 0, 0)\n",
    "epochs = 8\n",
    "\n",
    "lr_trial = [1e-2, 1e-3, 1e-4, 5e-5]\n",
    "batch_trial = [8, 16, 32]\n",
    "\n",
    "trial = 10\n",
    "\n",
    "#Round 1 Random Search (Coarse)\n",
    "for _ in range(trial):\n",
    "    lr = lr_trial[randint(0,3)]\n",
    "    batch = batch_trial[randint(0,2)]\n",
    "    print(lr, batch)\n",
    "    train_generator, validation_generator, test_generator = create_datagen(train_path, val_path, test_path,\n",
    "                                                                       batch_size = batch,efficient = True)\n",
    "    model = make_EfficientB0()\n",
    "    model.compile(loss = CategoricalCrossentropy(from_logits=False, label_smoothing = 0.2, axis=-1), \n",
    "                                optimizer = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, \n",
    "                                            epsilon=None, amsgrad=False), \n",
    "                               metrics = ['accuracy'])\n",
    "    hist = model.fit(\n",
    "            train_generator,\n",
    "            epochs = epochs,\n",
    "            validation_data=validation_generator,\n",
    "            verbose = 2\n",
    "        )\n",
    "    val_loss = np.nanmin(hist.history['val_loss'])\n",
    "    print(f\"Minimum val loss is {val_loss}\")\n",
    "    if val_loss < best_hyperparam[0]:\n",
    "        best_hyperparam = (val_loss, lr, batch)\n",
    "        print(best_hyperparam)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f181fdab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START RANDOM SEARCH ROUND 2 (FINE)\n",
      "Found 3480 images belonging to 4 classes.\n",
      "Found 101 images belonging to 4 classes.\n",
      "Found 106 images belonging to 4 classes.\n",
      "Epoch 1/5\n",
      "218/218 - 50s - loss: 1.4721 - accuracy: 0.5652 - val_loss: 1.5719 - val_accuracy: 0.2178 - 50s/epoch - 232ms/step\n",
      "Epoch 2/5\n",
      "218/218 - 34s - loss: 0.9715 - accuracy: 0.8193 - val_loss: 1.3973 - val_accuracy: 0.2970 - 34s/epoch - 157ms/step\n",
      "Epoch 3/5\n",
      "218/218 - 34s - loss: 0.8279 - accuracy: 0.9060 - val_loss: 1.0928 - val_accuracy: 0.7525 - 34s/epoch - 157ms/step\n",
      "Epoch 4/5\n",
      "218/218 - 34s - loss: 0.7726 - accuracy: 0.9362 - val_loss: 1.0791 - val_accuracy: 0.7624 - 34s/epoch - 157ms/step\n",
      "Epoch 5/5\n",
      "218/218 - 34s - loss: 0.7245 - accuracy: 0.9618 - val_loss: 1.1465 - val_accuracy: 0.6634 - 34s/epoch - 158ms/step\n",
      "Minimum val loss is 1.0791015625\n",
      "Epoch 1/5\n",
      "218/218 - 50s - loss: 1.3087 - accuracy: 0.6236 - val_loss: 2.0967 - val_accuracy: 0.2079 - 50s/epoch - 230ms/step\n",
      "Epoch 2/5\n",
      "218/218 - 34s - loss: 0.8060 - accuracy: 0.8951 - val_loss: 2.2228 - val_accuracy: 0.1188 - 34s/epoch - 155ms/step\n",
      "Epoch 3/5\n",
      "218/218 - 34s - loss: 0.7133 - accuracy: 0.9580 - val_loss: 1.9212 - val_accuracy: 0.1188 - 34s/epoch - 156ms/step\n",
      "Epoch 4/5\n",
      "218/218 - 34s - loss: 0.6826 - accuracy: 0.9759 - val_loss: 1.5919 - val_accuracy: 0.1188 - 34s/epoch - 155ms/step\n",
      "Epoch 5/5\n",
      "218/218 - 34s - loss: 0.6726 - accuracy: 0.9782 - val_loss: 2.0777 - val_accuracy: 0.1188 - 34s/epoch - 155ms/step\n",
      "Minimum val loss is 1.5918666294642858\n",
      "START RANDOM SEARCH ROUND 3 (VERY FINE)\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Epoch 1/5\n",
      "218/218 - 40s - loss: 1.5083 - accuracy: 0.4601 - val_loss: 1.2990 - val_accuracy: 0.6139 - 40s/epoch - 182ms/step\n",
      "Epoch 2/5\n",
      "218/218 - 27s - loss: 1.0075 - accuracy: 0.7655 - val_loss: 1.4032 - val_accuracy: 0.0495 - 27s/epoch - 123ms/step\n",
      "Epoch 3/5\n",
      "218/218 - 27s - loss: 0.8664 - accuracy: 0.8635 - val_loss: 1.3442 - val_accuracy: 0.5149 - 27s/epoch - 123ms/step\n",
      "Epoch 4/5\n",
      "218/218 - 27s - loss: 0.7916 - accuracy: 0.9204 - val_loss: 1.3018 - val_accuracy: 0.6139 - 27s/epoch - 123ms/step\n",
      "Epoch 5/5\n",
      "218/218 - 27s - loss: 0.7364 - accuracy: 0.9603 - val_loss: 1.3131 - val_accuracy: 0.4851 - 27s/epoch - 123ms/step\n",
      "Minimum val loss is 1.2989676339285714\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Epoch 1/5\n",
      "218/218 - 38s - loss: 1.4595 - accuracy: 0.5575 - val_loss: 1.3328 - val_accuracy: 0.6139 - 38s/epoch - 174ms/step\n",
      "Epoch 2/5\n",
      "218/218 - 26s - loss: 0.9011 - accuracy: 0.8511 - val_loss: 1.3203 - val_accuracy: 0.6139 - 26s/epoch - 120ms/step\n",
      "Epoch 3/5\n",
      "218/218 - 26s - loss: 0.8230 - accuracy: 0.8974 - val_loss: 1.2661 - val_accuracy: 0.6139 - 26s/epoch - 120ms/step\n",
      "Epoch 4/5\n",
      "218/218 - 26s - loss: 0.7451 - accuracy: 0.9557 - val_loss: 1.2590 - val_accuracy: 0.6139 - 26s/epoch - 120ms/step\n",
      "Epoch 5/5\n",
      "218/218 - 26s - loss: 0.7147 - accuracy: 0.9695 - val_loss: 1.2517 - val_accuracy: 0.6139 - 26s/epoch - 120ms/step\n",
      "Minimum val loss is 1.2517438616071428\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Epoch 1/5\n",
      "218/218 - 38s - loss: 1.2738 - accuracy: 0.6221 - val_loss: 1.8443 - val_accuracy: 0.1188 - 38s/epoch - 174ms/step\n",
      "Epoch 2/5\n",
      "218/218 - 26s - loss: 0.8775 - accuracy: 0.8517 - val_loss: 1.4738 - val_accuracy: 0.1188 - 26s/epoch - 121ms/step\n",
      "Epoch 3/5\n",
      "218/218 - 26s - loss: 0.7563 - accuracy: 0.9420 - val_loss: 1.3965 - val_accuracy: 0.1188 - 26s/epoch - 121ms/step\n",
      "Epoch 4/5\n",
      "218/218 - 26s - loss: 0.7081 - accuracy: 0.9707 - val_loss: 1.4208 - val_accuracy: 0.1287 - 26s/epoch - 121ms/step\n",
      "Epoch 5/5\n",
      "218/218 - 26s - loss: 0.6841 - accuracy: 0.9848 - val_loss: 1.4717 - val_accuracy: 0.1188 - 26s/epoch - 121ms/step\n",
      "Minimum val loss is 1.396484375\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Epoch 1/5\n",
      "218/218 - 38s - loss: 1.3469 - accuracy: 0.5825 - val_loss: 2.3379 - val_accuracy: 0.1188 - 38s/epoch - 174ms/step\n",
      "Epoch 2/5\n",
      "218/218 - 26s - loss: 0.8545 - accuracy: 0.8615 - val_loss: 2.5432 - val_accuracy: 0.1188 - 26s/epoch - 120ms/step\n",
      "Epoch 3/5\n",
      "218/218 - 26s - loss: 0.7433 - accuracy: 0.9463 - val_loss: 2.3326 - val_accuracy: 0.1188 - 26s/epoch - 119ms/step\n",
      "Epoch 4/5\n",
      "218/218 - 26s - loss: 0.7173 - accuracy: 0.9638 - val_loss: 2.2330 - val_accuracy: 0.1188 - 26s/epoch - 119ms/step\n",
      "Epoch 5/5\n",
      "218/218 - 26s - loss: 0.6883 - accuracy: 0.9833 - val_loss: 2.0399 - val_accuracy: 0.1188 - 26s/epoch - 119ms/step\n",
      "Minimum val loss is 2.0398995535714284\n"
     ]
    }
   ],
   "source": [
    "print(f\"START RANDOM SEARCH ROUND 2 (FINE)\")\n",
    "\n",
    "epochs = 5\n",
    "train_generator, validation_generator, test_generator = create_datagen(train_path, val_path, test_path,\n",
    "                                                                       batch_size = best_hyperparam[2],efficient = False)\n",
    "\n",
    "#Round 2 Random Search (Fine)\n",
    "lr_trial_2 = [ best_hyperparam[1] / 2, best_hyperparam[1] * 2 ]\n",
    "for lr in lr_trial_2:\n",
    "    model = make_EfficientB0()\n",
    "    model.compile(loss = CategoricalCrossentropy(from_logits=False, label_smoothing = 0.2, axis=-1), \n",
    "                                optimizer = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, \n",
    "                                            epsilon=None, amsgrad=False), \n",
    "                               metrics = ['accuracy'])\n",
    "    hist = model.fit(\n",
    "            train_generator,\n",
    "            epochs = epochs,\n",
    "            validation_data=validation_generator,\n",
    "            verbose = 2\n",
    "        )\n",
    "    val_loss = np.nanmin(hist.history['val_loss'])\n",
    "    print(f\"Minimum val loss is {val_loss}\")\n",
    "    if val_loss < best_hyperparam[0]:\n",
    "        best_hyperparam = (val_loss, lr, batch)\n",
    "        print(best_hyperparam)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "print(f\"START RANDOM SEARCH ROUND 3 (VERY FINE)\")\n",
    "#Round 3 Random Search (Very Fine)\n",
    "lr_trial_3 = [ best_hyperparam[1] / 3, best_hyperparam[1] * 2/3 ,  \n",
    "              best_hyperparam[1] * 4/3, best_hyperparam[1] * 5/3]\n",
    "for lr in lr_trial_3:\n",
    "    model = make_mobileNet()\n",
    "    model.compile(loss = CategoricalCrossentropy(from_logits=False, label_smoothing = 0.2, axis=-1), \n",
    "                                optimizer = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, \n",
    "                                            epsilon=None, amsgrad=False), \n",
    "                               metrics = ['accuracy'])\n",
    "    hist = model.fit(\n",
    "            train_generator,\n",
    "            epochs = epochs,\n",
    "            validation_data=validation_generator,\n",
    "            verbose = 2\n",
    "        )\n",
    "    val_loss = np.nanmin(hist.history['val_loss'])\n",
    "    print(f\"Minimum val loss is {val_loss}\")\n",
    "    if val_loss < best_hyperparam[0]:\n",
    "        best_hyperparam = (val_loss, lr, batch)\n",
    "        print(best_hyperparam)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "717c4aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7833426339285714, 0.001, 16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fde2db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
